{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Good beta value to start, as referenced by ritchieng.com\n",
    "beta = 0.01\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularisation for Logistic Model :  Better to no regularisation : 88.8% @1m2sec  vs 82.1% @58sec**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Logistic REgression with Regularisation initialisation complete1\n"
     ]
    }
   ],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  # loss function without regularisation\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    # Loss function with regularisation\n",
    "  regularisation_term = tf.nn.l2_loss(weights)\n",
    "  #loss = tf.reduce_mean(\n",
    "  #  tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta * regularisation_term\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta * regularisation_term\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "print('Simple Logistic REgression with Regularisation initialisation complete1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run NOW!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 47.543068\n",
      "Training accuracy: 9.1%\n",
      "Validation accuracy: 12.0%\n",
      "Loss at step 100: 11.430303\n",
      "Training accuracy: 74.1%\n",
      "Validation accuracy: 71.8%\n",
      "Loss at step 200: 4.330813\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 76.7%\n",
      "Loss at step 300: 1.927941\n",
      "Training accuracy: 82.7%\n",
      "Validation accuracy: 79.6%\n",
      "Loss at step 400: 1.111113\n",
      "Training accuracy: 84.2%\n",
      "Validation accuracy: 81.1%\n",
      "Loss at step 500: 0.830003\n",
      "Training accuracy: 84.8%\n",
      "Validation accuracy: 81.8%\n",
      "Loss at step 600: 0.732112\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 82.1%\n",
      "Loss at step 700: 0.697701\n",
      "Training accuracy: 85.0%\n",
      "Validation accuracy: 82.2%\n",
      "Loss at step 800: 0.685507\n",
      "Training accuracy: 84.9%\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.9%\n",
      "Simple Logit regression with tensor graph run time  0:00:56.670356\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "start_run = datetime.datetime.now()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  #tf.global_variables_initializer().run() ## using old tensorflow hence switching to old variable intialisation approach \n",
    "  tf.initialize_all_variables().run()  \n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "print('Simple Logit regression with tensor graph run time ', datetime.datetime.now() - start_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Regularisation for SGD  Model  : Better  **     \n",
    "-----------------------------------------------\n",
    "** Regularised SGD vs SGD ::  88.6%@22s vs 86.9% @9s   **                 \n",
    "** Regularised SGD vs GD  ::  88.6%@22s vs 88.8% @1m2sec **                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Initialisation Complete\n"
     ]
    }
   ],
   "source": [
    "def init_SGD(batch_size = 128):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "      tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(batch_size, image_size * image_size))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "      # Variables.\n",
    "      weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "      biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "      # Training computation.\n",
    "      logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "      loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "      # Optimizer.\n",
    "      optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "      test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    print('SGD Initialisation Complete')\n",
    "\n",
    "batch_size = 128\n",
    "init_SGD(batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run NOW !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (128, 784) for Tensor u'Const:0', which has shape '(10000, 784)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b7ea0eba5a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrun_SGD\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;31m#run_SGD( num_steps = num_steps , limit_train_size_to = 512 )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-b7ea0eba5a02>\u001b[0m in \u001b[0;36mrun_SGD\u001b[0;34m(num_steps, limit_train_size_to)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         _, l, predictions = session.run(\n\u001b[0;32m---> 26\u001b[0;31m           [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    888\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (128, 784) for Tensor u'Const:0', which has shape '(10000, 784)'"
     ]
    }
   ],
   "source": [
    "def run_SGD(num_steps = 3001, limit_train_size_to = ''):\n",
    "    if limit_train_size_to == '':\n",
    "        # consider all train data\n",
    "        limit_train_size_to = train_dataset.shape[0]\n",
    "    train_dataset_limited_to = train_dataset[0:limit_train_size_to,:]\n",
    "    train_labels_limited_to = train_labels[0:limit_train_size_to,:]\n",
    "    accuracy_all = np.ndarray(shape = (1 + num_steps/500, 4), dtype = np.float32)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      #tf.global_variables_initializer().run()\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels_limited_to.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset_limited_to[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels_limited_to[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    print('Training time taken with stochastic gradient and tensorflow', datetime.datetime.now()-start)\n",
    "\n",
    "num_steps = 3001\n",
    "run_SGD( num_steps = num_steps )\n",
    "#run_SGD( num_steps = num_steps , limit_train_size_to = 512 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Regularised Deep Learning. Neural network - 1 hidden network ( 1024 hidden nodes)**\n",
    "------------------------------------------------------------------------------\n",
    "** Better and More Robust to SGD**                      \n",
    "** Regularised NN vs NN : 90.3% @2.23m vs 89.3% @3.15m  **             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_accuracy(data):\n",
    "    figure, axis = plt.subplots()\n",
    "    axis.plot(data[:,0], data[:, 1:4] )\n",
    "    axis.set_title('Accuracy across diff training size')\n",
    "    axis.legend(('training batch data','validation data', 'test data'), loc = 'lower right')\n",
    "    axis.set_xticks(data[:,0]) # set batch size on x -axis\n",
    "    axis.set_xlabel('')\n",
    "    axis.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "collect_accuracy_at_batch_interval_of = 500\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_NN(batch_size,     num_hidden,    beta):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "\n",
    "      # Input data. Unlike in earlier gradient descent, now we can take the whole data.\n",
    "      # With gradient descent, we could not process all the input data and we had poor accuracy\n",
    "      # For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "      tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                        shape=(batch_size, image_size * image_size))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "      # Variables.\n",
    "      weights_for_hidden = tf.Variable(tf.truncated_normal([ image_size * image_size , num_hidden]))\n",
    "      biases_for_hidden = tf.Variable(tf.zeros([num_hidden]))\n",
    "      weights = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden, num_labels]))\n",
    "      biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "      # Training computation.\n",
    "      hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_for_hidden ) + biases_for_hidden)\n",
    "      hidden_valid_dataset = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_for_hidden) + biases_for_hidden)\n",
    "      hidden_test_dataset = tf.nn.relu(tf.matmul(tf_test_dataset, weights_for_hidden) + biases_for_hidden)\n",
    "      #logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "      logits = tf.matmul(hidden1, weights) + biases\n",
    "      # loss without regularisation\n",
    "      loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "      ## loss with regularisation\n",
    "      regularisation_term = tf.nn.l2_loss(weights_for_hidden) + tf.nn.l2_loss(weights)\n",
    "      loss = tf.reduce_mean( loss + beta * regularisation_term)\n",
    "\n",
    "      # Optimizer.\n",
    "      optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)  \n",
    "\n",
    "      # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(hidden_valid_dataset, weights) + biases)\n",
    "      test_prediction = tf.nn.softmax(tf.matmul(hidden_test_dataset, weights) + biases)\n",
    "    print('Deep Neural Network with 1 Hidden layer with 1024 hidden nodes initialisation complete', datetime.datetime.now())\n",
    "\n",
    "\n",
    "init_NN( batch_size = 128,     num_hidden = 1024,    beta = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run NOW !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_dataset.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_NN(limit_train_size_to  = ''):\n",
    "    num_steps = 3001\n",
    "    if limit_train_size_to == '':\n",
    "        # consider all train data\n",
    "        limit_train_size_to = train_dataset.shape[0]\n",
    "    train_dataset_limited_to = train_dataset[0:limit_train_size_to,:]\n",
    "    train_labels_limited_to = train_labels[0:limit_train_size_to,:]\n",
    "    accuracy_all = np.ndarray(shape = (1 + num_steps/collect_accuracy_at_batch_interval_of, 4), dtype = np.float32)\n",
    "    offset_range = set()\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized\")\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels_limited_to.shape[0] - batch_size)\n",
    "        offset_range.add(str(offset) + '-' + str(offset + batch_size))\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset_limited_to[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels_limited_to[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % collect_accuracy_at_batch_interval_of == 0):\n",
    "          train_batch_accuracy = accuracy(predictions, batch_labels)\n",
    "          validation_accuracy = accuracy( valid_prediction.eval(), valid_labels)\n",
    "          test_accuracy = accuracy(test_prediction.eval(), test_labels) \n",
    "          accuracy_all[step /collect_accuracy_at_batch_interval_of, :  ] = [step * batch_size, train_batch_accuracy, \n",
    "                                                          validation_accuracy, test_accuracy  ]\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % train_batch_accuracy)\n",
    "          print(\"Validation accuracy: %.1f%%\" % validation_accuracy )\n",
    "      print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    print(' Following training sets has been tried . Range Set Size',len(offset_range) )\n",
    "    print(' Training with deep neural netowrk 1 hidden layer, 1024 node. Time taken ', datetime.datetime.now() - start )\n",
    "    #print(accuracy_all)\n",
    "    plot_accuracy(accuracy_all)\n",
    "    print('. Sample 100 Ranges are : ', list(offset_range)[0:100] )\n",
    "\n",
    "run_NN()\n",
    "#run_NN(limit_train_size_to = 512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small Training Size vs SGD \n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run SGD with small training size\n",
    "# See how it performs, compared to NN for the same data size\n",
    "run_SGD( num_steps = num_steps , limit_train_size_to = 512 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small Training Size vs Regularised Neural Network\n",
    "-----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_NN( batch_size = 128, num_hidden = 1024, beta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "num_steps = 3001\n",
    "# restricted batch size i.e training example\n",
    "small_sample_size = 512\n",
    "train_dataset_small = train_dataset[0:small_sample_size,:]\n",
    "print('train small dataset shape is ',train_dataset_small.shape)\n",
    "train_labels_small = train_labels[0:small_sample_size,:]\n",
    "offset_range = set()\n",
    "\n",
    "accuracy_all_small = numpy.ndarray(shape = (1 + num_steps / collect_accuracy_at_batch_interval_of,4) , dtype = np.float32 )\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels_small.shape[0] - batch_size)\n",
    "    offset_range.add(str(offset) + '-' + str(offset + batch_size))\n",
    "    #print('offset is ', offset, offset + batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset_small[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels_small[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % collect_accuracy_at_batch_interval_of == 0):\n",
    "      train_batch_accuracy = accuracy(predictions, batch_labels)\n",
    "      validation_accuracy = accuracy( valid_prediction.eval(), valid_labels)\n",
    "      test_accuracy = accuracy(test_prediction.eval(), test_labels) \n",
    "      accuracy_all_small[step /collect_accuracy_at_batch_interval_of, :  ] \\\n",
    "        = [step * batch_size, train_batch_accuracy, validation_accuracy, test_accuracy ]    \n",
    "\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % train_batch_accuracy)\n",
    "      print(\"Validation accuracy: %.1f%%\" % validation_accuracy )\n",
    "  print(\"Test accuracy: %.1f%%\" % test_accuracy )\n",
    "print('Following training sets has been  used for the training purpose ',offset_range)\n",
    "print('Training with Restricted data size , batch sizedeep neural netowrk 1 hidden layer, 1024 node. Time taken ', datetime.datetime.now() - start )\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run_NN(limit_train_size_to = 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFwCAYAAAAyp+hsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb91406cf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_accuracy(accuracy_all)\n",
    "#print(accuracy_all_small)\n",
    "plot_accuracy(accuracy_all_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion : ** We observed that, when few train datasets (i.e 500) are provided, then the Minibatch accuracy is 100% across each steps, while the validation accuracy is 78.5%. This indicates the extreme case of overfitting, where our model is completely able to predict across training dataset with 100% correctness, however when faced with new data as in the validation set,  the performance significantly drops. This may be explained by the fact that  the only training set is ['256-384', '128-256', '0-128'] index data of the all available.  \n",
    "\n",
    "Compare this to our earlier cases, where we used the full dataset and our training samples were 3000 different training examples, the minibatch accuracy was never vastly different to the validation accurayc and never greater than 89%, which meant that the model generated from the training was never overfitting. And that the train model also performed with fairly similar performance across train and validation set meant that the model was generic enough to work even for the unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN with Dropout, now regularisation\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_hidden = 1024\n",
    "beta = 0.01\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. Unlike in earlier gradient descent, now we can take the whole data.\n",
    "  # With gradient descent, we could not process all the input data and we had poor accuracy\n",
    "  # For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  # Variables.\n",
    "  weights_for_hidden = tf.Variable(tf.truncated_normal([ image_size * image_size , num_hidden]))\n",
    "  biases_for_hidden = tf.Variable(tf.zeros([num_hidden]))\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "  hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_for_hidden ) + biases_for_hidden)\n",
    "  hidden_valid_dataset = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_for_hidden) + biases_for_hidden)\n",
    "  hidden_test_dataset = tf.nn.relu(tf.matmul(tf_test_dataset, weights_for_hidden) + biases_for_hidden)\n",
    "  #logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  \n",
    "  logits = tf.matmul(hidden1, weights) + biases\n",
    "  # loss without regularisation\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  ## loss with regularisation\n",
    "  regularisation_term = tf.nn.l2_loss(weights_for_hidden) + tf.nn.l2_loss(weights)\n",
    "  loss = tf.reduce_mean( loss + beta * regularisation_term)\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)  \n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(hidden_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(hidden_test_dataset, weights) + biases)\n",
    "print('Deep Neural Network with 1 Hidden layer with 1024 hidden nodes initialisation complete', datetime.datetime.now())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
